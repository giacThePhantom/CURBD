{"cmd": "r\"\"\"°°°\n# Recurrent spiking neural networks\nThis notebook provides an example in how to:\n    * Set up a recurrent-spiking neural network\n    * Generate data to give to it as input\n    * Train the network so that it matches the data\n\nSources for this notebook include:\n    * [The CURBD tool](https://github.com/rajanlab/CURBD)\n°°°\"\"\"\n# |%%--%%| <NEH5SUEzp5|QhYxSMc5Z6>\nr\"\"\"°°°\n# Creating the network\n°°°\"\"\"\n# |%%--%%| <QhYxSMc5Z6|mantena>\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.random as npr\nimport math\n\n# |%%--%%| <mantena|L7xcJxIvJC>\nr\"\"\"°°°\n## Neurons\nNeurons are modeled by the time evolution of their membrane potential $V$:\n\n$$\\tau\\frac{dV_{i}}{dt} = -V + g\\sum\\limits_{j}J_{ij}\\phi(V_{j}) + I_{i}$$\n\nThis differential equation is solved using the Euler method, so the evolution of the voltage from one timestep to another is:\n\n$$V_{i}(t + \\Delta t) = V_{i}(t) + \\frac{-V_{i}(t) + g\\sum\\limits_{j}J_{ij}\\phi(V_{j}(t)) + I_{i}(t)}{\\tau}\\Delta t$$\n°°°\"\"\"\n# |%%--%%| <L7xcJxIvJC|TpmXKvZIwD>\n\nn_neurons = 300\ng = 1.5 #Neuron's conductance\ntau_neuron = 0.1 #Neuron's time constant\n\ndef init_neurons(n_neurons):\n    return npr.normal(\n            -0.7, #Minimum value\n            0.7, #Maximum value\n            (n_neurons, 1) #(n_neurons, n_timesteps)\n            )\n\ninit_neurons(n_neurons)\n\n# |%%--%%| <TpmXKvZIwD|jahVKoLM65>\n\ndef get_activity(V, phi):\n    return phi(V)\n\ndef update_voltage(V, g, J, I, activity, tau, dt):\n    return V + (-V + g * J.dot(activity) + I) * dt / tau\n\n# |%%--%%| <jahVKoLM65|xN8JsfYrEi>\nr\"\"\"°°°\n## Synapses\nThe synapses are represented as a matrix $J$ of size $n \\times n$. where $n$ is the number of neurons.\nThe value of $J_{ij}$ represents the strength of the connection between neuron $i$ and neuron $j$, such that:\n* $J_{ij} > 0$ means that neuron $i$ excites neuron $j$.\n* $J_{ij} < 0$ means that neuron $i$ inhibits neuron $j$.\n* $J_{ij} = 0$ means that there is no connection between neuron $i$ and neuron $j$.\n\nThe matrix is initialized with random values distributed as a normal distribution and then normalized according to the number of neurons $n$:\n\n$$J_{ij} \\sim \\frac{N(0, 1)}{\\sqrt{n}}$$\n°°°\"\"\"\n# |%%--%%| <xN8JsfYrEi|tZpgGDHTAA>\n\ndef init_synapses(n_neurons):\n    return npr.randn(n_neurons, n_neurons) / math.sqrt(n_neurons)\ninit_synapses(n_neurons)\n\n# |%%--%%| <tZpgGDHTAA|DHjIt9fgCu>\n\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\nsubplot.imshow(init_synapses(n_neurons))\n\n# |%%--%%| <DHjIt9fgCu|N2x7xaeO4F>\nr\"\"\"°°°\n## Input\nThe input I is computed as:\n\n$$I_{i}(t) = \\eta_{i}(t) + (I_{i}(t - \\Delta t) - \\eta_{i}(t))e^{-\\frac{\\Delta t}{\\tau}}$$\n\nWhere $\\eta_{i}(t)$ is a white noise process with a time constant $\\tau$.\n\nThis is done to ensure that the input of different neurons is not correlated.\n°°°\"\"\"\n# |%%--%%| <N2x7xaeO4F|Qcda8uk2iM>\n\ntau_input = 0.1 #The time constant of the input\ndt = 0.01 / 5 #The timestep of the input\nscale_input = 0.01 #The scale of the input\n\ndef compute_input(n_neurons, tau_input, scale_input, dt, sim_time):\n    n_timesteps = int(np.ceil(sim_time / dt)) #The number of timesteps\n    eta = npr.randn(n_neurons, n_timesteps) * math.sqrt(tau_input / dt) #The noise process\n    I = np.ones((n_neurons, n_timesteps)) #Initializing the input\n\n    for i in range(1, n_timesteps): #Computing the input\n        I[:, i] = eta[:, i] + (I[:, i - 1] - eta[:, i]) * np.exp(-dt / tau_input)\n\n    return I * scale_input\n\nI = compute_input(300, tau_input, scale_input, dt, 12)\n\n# |%%--%%| <Qcda8uk2iM|c0YKNrAEh4>\n\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\n\nsubplot.plot(I[0, :])\nsubplot.set_xlabel(\"Time steps\")\nsubplot.set_ylabel(\"Input current\")\n\n# |%%--%%| <c0YKNrAEh4|FwK2GquHuk>\n\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\n\nsubplot.imshow(I, aspect = 'auto')\nsubplot.set_xlabel(\"Time steps\")\nsubplot.set_ylabel(\"Neurons\")\n\n# |%%--%%| <FwK2GquHuk|rkJqlKN2rh>\nr\"\"\"°°°\n## Activation function\nThe activation function is any non-linear function like:\n* The sigmoid.\n* The hyperbolic tangent.\n* The rectified linear unit (ReLU)\n\nIn this case the hyperbolic tangent is used:\n\n$$\\phi(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n\nWith its derivative:\n\n$$\\phi'(x) = 1 - \\tanh^{2}(x) = 1 - \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}^{2}$$\n°°°\"\"\"\n# |%%--%%| <rkJqlKN2rh|L1DW3X5I98>\n\nphi = np.tanh\n\n# |%%--%%| <L1DW3X5I98|fcbTSOnoRZ>\n\nx = np.linspace(-5, 5, 1000)\ny = phi(x)\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\nsubplot.spines['left'].set_position('center')\nsubplot.spines['bottom'].set_position('center')\nsubplot.spines['right'].set_color('none')\nsubplot.spines['top'].set_color('none')\nsubplot.xaxis.set_ticks_position('bottom')\nsubplot.yaxis.set_ticks_position('left')\nsubplot.set_title(\"Activation function\")\nsubplot.plot(x, y)\n\n# |%%--%%| <fcbTSOnoRZ|nsVBtXprrj>\n\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\nsubplot.spines['left'].set_position('center')\nsubplot.spines['bottom'].set_position('center')\nsubplot.spines['right'].set_color('none')\nsubplot.spines['top'].set_color('none')\nsubplot.xaxis.set_ticks_position('bottom')\nsubplot.yaxis.set_ticks_position('left')\nsubplot.set_title(\"Derivative of the activation function\")\nsubplot.plot(x, 1 - y**2)\n\n# |%%--%%| <nsVBtXprrj|vzC7GmU5JD>\nr\"\"\"°°°\n# Training\nTraining consists of updating the strength of the synapses connections $J_{ij}$ so that the activity of the neurons is as close as possible to the one found in the experimental data.\nThis is done by computing the error between the experimental data and the output of the network and then updating the synapses connections according to the error:\n\n$$J_{ij}(t) = J_{ij}(t-1) + \\Delta J_{ij}(t)$$\n\n$$\\Delta J_{ij}(t) = c\\cdot e_{i}(t)\\sum\\limits_{k=1}^N P_{jk}(t)\\phi_{k}(t)$$\n°°°\"\"\"\n# |%%--%%| <vzC7GmU5JD|MQgpY2zxrQ>\nr\"\"\"°°°\n## Error function\nThe error function is the difference between the experimental data and the output of the network:\n\n$$e_{i}(t) = z_{i}(t) - a_{i}(t)$$\n°°°\"\"\"\n# |%%--%%| <MQgpY2zxrQ|OHXncPsWTH>\n\ndef get_error(simulated, experimental):\n    return simulated - experimental\n\n# |%%--%%| <OHXncPsWTH|FWsNOtQM3m>\nr\"\"\"°°°\n## Inverse cross-correlation matrix\n$P$ is the probability that neuron $j$ will fire in the next timestep, so the inverse cross-correlation matrix of the firing rates of units in the network.\n\n$$P_{ij} = \\langle\\phi_{i}\\phi_{j}\\rangle^{-1}$$\n\nThis is costly to compute at every timestep, so it is computed iteratively as:\n\n$$P(t) = P(t-1) - c\\cdot ||(P(t-1)\\cdot\\phi(t))|| = P(t-1) - c\\cdot((P(t-1)\\cdot\\phi(t))(P(t-1)\\cdot\\phi(t))^T)$$\n°°°\"\"\"\n# |%%--%%| <FWsNOtQM3m|wwvyiUURv6>\n\nlearning_rate = 1.0 #Works best when set to be 1 to 10 times the overall amplitude of the external inputs\n\ndef init_P(n_neurons, learning_rate):\n    return learning_rate * np.eye(n_neurons)\n\n# |%%--%%| <wwvyiUURv6|qWKW82G6gz>\n\ndef get_activity_P(P, activity):\n    return P.dot(activity)\n\n# |%%--%%| <qWKW82G6gz|ERuwWeRy9a>\n\ndef update_P(P, activity_P, c):\n    return P - c * activity_P.dot(activity_P.T)\n\n# |%%--%%| <ERuwWeRy9a|TLRdRxgdiJ>\nr\"\"\"°°°\n## Scaling term\nThe scaling term $c$ scales the update of the synapses connections.\nIt is computed as:\n\n$c = \\frac{1}{1+\\phi^{T}(t)\\cdot(P(t-1)\\cdot\\phi(t))}$\n°°°\"\"\"\n# |%%--%%| <TLRdRxgdiJ|h78ov2I9Ex>\n\ndef get_c(activity_P, activity):\n    aPa = activity.T.dot(activity_P).item() # The result is a singleton, item extracts the number\n    return 1 / (1 + aPa)\n\n# |%%--%%| <h78ov2I9Ex|NTAIKNIRyt>\nr\"\"\"°°°\n## Update of the synapses Matrix\nRecall:\n\n$$\\Delta J_{ij}(t) = c\\cdot e_{i}(t)\\sum\\limits_{k=1}^N P_{jk}(t)\\phi_{k}(t)$$\n°°°\"\"\"\n# |%%--%%| <NTAIKNIRyt|4aDnFrdZTl>\n\ndef update_J(J, activity_P, error, c, target_neurons):\n    return J[:, target_neurons] - c * np.outer(error.flatten(), activity_P.flatten())\n\n# |%%--%%| <4aDnFrdZTl|7QIDJja5kb>\nr\"\"\"°°°\n## Training function\nNow that all the function have been defined, they can be collected in a single function that performs the training of the network.\n°°°\"\"\"\n# |%%--%%| <7QIDJja5kb|by855Yt3K7>\n\ndef train(simulated, experimental, J, P, target_neurons):\n    error = get_error(simulated, experimental)\n    activity_P = get_activity_P(P, simulated[target_neurons,:])\n    c = get_c(activity_P, simulated[target_neurons,:])\n    P = update_P(P, activity_P, c)\n    J[:, target_neurons.flatten()] = update_J(J, activity_P, error, c, target_neurons)\n    return J, P, np.mean(error ** 2)\n\n# |%%--%%| <by855Yt3K7|agBdu0TvNR>\nr\"\"\"°°°\n# The experimental data\nTo demonstrate the capabilities of the network the experimental data will be generated in silico as well.\nThe data will be generated according to a three region ground truth model.\nEach of the tree regions will contain $100$ neurons and will be driven by a different type of input:\n\n* The first region (A) will be driven by only by the recurrent inuts from the other two regions.\n* The second region (B) will be driven by a network generating a Gaussian \"bump\" propagating agross the network:\n* The final region (C) will be driven by a network generating a fixed point that is shifted then to another during the generation.\nThe fixed points are generated by sampling $SQ_{i}(t)$ at two differend time points and holding them at the sampled value for the duration of the fixed point.\n\nExternal inputs are connected to half of the units in their respective regions with a fixed negative weight (inhibitory) for region B and positive (excitatory) for region C.\n°°°\"\"\"\n# |%%--%%| <agBdu0TvNR|2LNjZRuYSx>\nr\"\"\"°°°\n## Generating the external inputs\n°°°\"\"\"\n# |%%--%%| <2LNjZRuYSx|CCkp3w5YEQ>\nr\"\"\"°°°\n### The Gaussian bump\n$$ SQ_{i}(t) = e^{-\\frac{1}{2\\sigma^2}\\left(\\frac{1-\\sigma-Nt}{T}\\right)^2}$$\nWhere $\\sigma$ is the width of the bump across the population, $N$ the population size and $t$ the total simulation time.\n°°°\"\"\"\n# |%%--%%| <CCkp3w5YEQ|vF7CwS320T>\n\ndef compute_x_bump(sim_time, dt, n_neurons, width, cutoff_traslation):\n    \"\"\"Selects which neurons have a higher firing rate.\n    They will be sampled from a Gaussian distrubtion\n    \"\"\"\n    n_samples = int(math.ceil(sim_time / dt))\n    t_data = np.arange(0, sim_time, dt)\n    x_bump = np.zeros((n_neurons, n_samples))\n    n_width = width * n_neurons #from percentage to number\n    norm = 2*n_width **2\n    cutoff = math.ceil(n_samples / 2) + cutoff_traslation\n\n    for i in range(n_neurons):\n        x_bump[i, :] = np.exp(- ((i - n_width - n_neurons * t_data / (t_data[-1] / 2)) **2 / norm))\n        x_bump[i, cutoff:] = x_bump[i, cutoff]\n\n    return x_bump\n\n# |%%--%%| <vF7CwS320T|zoK9rq0cf4>\nr\"\"\"°°°\n### Sequence driving network\nThe sequence driving network is a network that generates a fixed point that is shifted during the simulation in a sequenctial manner across neurons.\n°°°\"\"\"\n# |%%--%%| <zoK9rq0cf4|d9NCDdW6KB>\n\ndef sequence_driving_network(x_bump, dt , lead_time):\n    hbump = np.log((x_bump + 0.01)/(1-x_bump+0.01))\n    hbump = hbump - np.min(hbump)\n    hbump = hbump / np.max(hbump)\n    newmat = np.tile(hbump[:, 1, np.newaxis], (1, math.ceil(lead_time / dt)))\n    hbump = np.concatenate((newmat, hbump), axis=1)\n    return hbump\n\n# |%%--%%| <d9NCDdW6KB|UK5W1tRRRH>\n\nx_bump = compute_x_bump(10, 0.01, 100, 0.1, -10)\ntest = sequence_driving_network(x_bump, 0.01, 2)\nfigure, subplot = plt.subplots(1)\nplt.pcolormesh(np.arange(0, 12, 0.01), np.arange(0, 100), test)\n\n# |%%--%%| <UK5W1tRRRH|gy9STCGxnN>\nr\"\"\"°°°\n### Fixed point driving network\nThe fixed point driving network gnerates a fixed point that it is kept fixed until a certain point during the simulation, where it is translated to another neuron.\n°°°\"\"\"\n# |%%--%%| <gy9STCGxnN|Mi4GDataJz>\n\ndef fixed_point_driver(xbump, sim_time, lead_time, dt, n_neurons, cutoff_traslation, before_shift, after_shift):\n    \"\"\"Generates a fixed point that is shifted during the simulation,\n    cutoff_translation: where the shift happen\n    before_shift where to pick the current from x_bump before the shift\n    after_shift where to pick the current from x_bump after the shift\n    \"\"\"\n    n_samples = int(math.ceil((sim_time - lead_time) / dt))\n    x_fp = np.zeros(xbump.shape)\n    cutoff = math.ceil(n_samples / 2) + cutoff_traslation\n    for i in range(n_neurons):\n        front = xbump[i, before_shift] * np.ones((1, cutoff))\n        back = xbump[i, after_shift] * np.ones((1, n_samples - cutoff))\n        x_fp[i, :] = np.concatenate((front, back), axis = 1)\n    h_fp = np.log((x_fp + 0.01)/(1-x_fp+0.01))\n    h_fp -= np.min(h_fp)\n    h_fp = h_fp / np.max(h_fp)\n    newmat = np.tile(h_fp[:, 1, np.newaxis], (1, math.ceil(lead_time/dt)))\n    h_fp = np.concatenate((newmat, h_fp), axis=1)\n    return h_fp\n\n# |%%--%%| <Mi4GDataJz|XSIfrLRk46>\n\ntest = fixed_point_driver(x_bump, 12, 2, 0.01, 100, -10, 10, 300)\nfigure, subplot = plt.subplots(1)\nplt.pcolormesh(np.arange(0, 12, 0.01), np.arange(0, 100), test)\n\n# |%%--%%| <XSIfrLRk46|ns2PdFjA1O>\nr\"\"\"°°°\n## Connectivity\n°°°\"\"\"\n# |%%--%%| <ns2PdFjA1O|Qbdg5h3HF7>\nr\"\"\"°°°\n### Intra-region connectivity\nEach of the three regions will have random inter-connections.\nThe strength of the connection is sampled by a normalized normal distribution:\n\n$$J_{ij} \\sim \\mathcal{N}(0, \\frac{1}{\\sqrt{N_{in}}})$$\n\nAnd then scaled by a chaos factor $g$:\n\n$$J_{ij} = g\\cdot J_{ij}$$\n\n$g$ has to be sufficiently large so to facilitate chaotic dynamics in the network.\n°°°\"\"\"\n# |%%--%%| <Qbdg5h3HF7|xlXbPekh9D>\n\ndef intra_region_connectivity(n_neurons, g):\n    J = npr.randn(n_neurons, n_neurons)\n    J = (g / np.sqrt(n_neurons)) * J\n    return J\n\n# |%%--%%| <xlXbPekh9D|Q0M8UfFt7V>\n\nJ_a = intra_region_connectivity(100, 1.8)\nJ_b = intra_region_connectivity(100, 1.5)\nJ_c = intra_region_connectivity(100, 1.5)\n\nfigure, subplot = plt.subplots(1, 3)\nsubplot[0].imshow(J_a, aspect = 'auto')\nsubplot[0].set_title(\"RNN-A synaptic strenght\")\nsubplot[1].imshow(J_b, aspect = 'auto')\nsubplot[1].set_title(\"RNN-B synaptic strenght\")\nsubplot[2].imshow(J_c, aspect = 'auto')\nsubplot[2].set_title(\"RNN-C synaptic strenght\")\n\n# |%%--%%| <Q0M8UfFt7V|XC1z8bZenO>\nr\"\"\"°°°\n### Inter-region connectivity\nThe number of connection is controlled by the `frac_inter_reg`, which controls the percentage of connected neurons\n°°°\"\"\"\n# |%%--%%| <XC1z8bZenO|R9NUMkejtt>\n\ndef source_region_neurons_connected(n_neurons, connected_fraction):\n    \"\"\"Returns the array source_neurons of the neurons\n    in the source population, where source_neurons[i] = 1 if\n    neuron i is connected to the population and 0 otherwise\n    \"\"\"\n    n_connected = int(n_neurons * connected_fraction)\n    source_neurons = np.zeros((n_neurons, 1))\n    random_connected_neurons = npr.permutation(n_neurons)\n    source_neurons[random_connected_neurons[0:n_connected]] = 1\n    return source_neurons\n\n# |%%--%%| <R9NUMkejtt|h3uOj5FdsB>\n\nfrac_inter_reg = 0.05\nconnection_A_to_B = source_region_neurons_connected(100, frac_inter_reg)\nconnection_A_to_C = source_region_neurons_connected(100, frac_inter_reg)\nconnection_B_to_A = source_region_neurons_connected(100, frac_inter_reg)\nconnection_B_to_C = source_region_neurons_connected(100, frac_inter_reg)\nconnection_C_to_B = source_region_neurons_connected(100, frac_inter_reg)\nconnection_C_to_A = source_region_neurons_connected(100, frac_inter_reg)\n\nfrac_external_reg = 0.5\n\nconnection_sequence_to_B = source_region_neurons_connected(100, frac_external_reg)\nconnection_fixed_to_C = source_region_neurons_connected(100, frac_external_reg)\n\nprint(\"The number of connection between A and B is: \", np.sum(connection_A_to_B))\nprint(\"The number of connection between the sequence driver and B is: \", np.sum(connection_sequence_to_B))\n\n# |%%--%%| <h3uOj5FdsB|tl4zcmyqGm>\nr\"\"\"°°°\n## Initial state\nThe initial state is a `n_neurons` vector of values sampled from a normal distribution.\n°°°\"\"\"\n# |%%--%%| <tl4zcmyqGm|WjuzEEzOvb>\n\ndef get_initial_state(n_neurons):\n    return 2 * npr.rand(n_neurons, 1) - 1\n\n# |%%--%%| <WjuzEEzOvb|7WRHXhPqwm>\nr\"\"\"°°°\n## Generating the data\nTo generate the data a number of parameters is necessary, so they are all collected in a dictionary.\n°°°\"\"\"\n# |%%--%%| <7WRHXhPqwm|teucre>\n\ndata_params = {\n        'sim_time' : 12,\n        'lead_time' : 2,\n        'dt' : 0.01,\n        'bump_width' : 0.2,\n        'n_neurons_A' : 100,\n        'n_neurons_B' : 100,\n        'n_neurons_C' : 100,\n        'g_A' : 1.8,\n        'g_B' : 1.5,\n        'g_C' : 1.5,\n        'frac_inter_reg' : 0.05,\n        'frac_ext_reg' : 0.5,\n        'activation_function' : np.tanh,\n        'tau' : 0.1,\n        'amplitude_inter_region' : 0.02,\n        'amplitude_sequential' : 1,\n        'amplitude_fixed' : -1,\n        'cutoff_traslation' : 100,\n        'before_shift' : 10,\n        'after_shift' : 300,\n        }\n\nn_datapoints = int(data_params['sim_time'] / data_params['dt'])\ndata = {}\nconnections = {}\n\n# |%%--%%| <teucre|lVJAoZSZ3Z>\nr\"\"\"°°°\n### Setting up all the necessary variables\n°°°\"\"\"\n# |%%--%%| <lVJAoZSZ3Z|NQycjuSucJ>\n\nx_bump = compute_x_bump(\n        data_params['sim_time'] - data_params['lead_time'],\n        data_params['dt'],\n        data_params['n_neurons_B'],\n        data_params['bump_width'],\n        -data_params['cutoff_traslation'],\n        )\n\ndata = {\n        'A' : np.empty((data_params['n_neurons_A'], n_datapoints)),\n        'B' : np.empty((data_params['n_neurons_B'], n_datapoints)),\n        'C' : np.empty((data_params['n_neurons_C'], n_datapoints)),\n        'sequential' : sequence_driving_network(\n            x_bump,\n            data_params['dt'],\n            data_params['lead_time']\n            ),\n        'fixed' : fixed_point_driver(\n            x_bump,\n            data_params['sim_time'],\n            data_params['lead_time'],\n            data_params['dt'],\n            data_params['n_neurons_C'],\n            data_params['cutoff_traslation'],\n            data_params['before_shift'],\n            data_params['after_shift'],\n            ),\n        }\n\ndata['A'][:] = np.NaN\ndata['B'][:] = np.NaN\ndata['C'][:] = np.NaN\n\nstates = {\n        'A' : get_initial_state(data_params['n_neurons_A']),\n        'B' : get_initial_state(data_params['n_neurons_B']),\n        'C' : get_initial_state(data_params['n_neurons_C']),\n        }\nconnections['A'] = {\n        'A' : {\n            'J' : intra_region_connectivity(\n                data_params['n_neurons_A'],\n                data_params['g_A']\n                ),\n            },\n        'B' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_A'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'C' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_A'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        }\nconnections['B'] = {\n        'A' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_B'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'B' : {\n            'J' : intra_region_connectivity(\n                data_params['n_neurons_B'],\n                data_params['g_B']\n                ),\n            },\n        'C' : {\n            'region' : 'C',\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_B'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'sequential' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_B'],\n                data_params['frac_ext_reg']\n                ),\n            'w' : data_params['amplitude_sequential'],\n            },\n        }\nconnections['C'] = {\n        'A' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_C'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'B' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_C'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'C' : {\n            'J' : intra_region_connectivity(\n                data_params['n_neurons_C'],\n                data_params['g_C']\n                ),\n        },\n        'fixed' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_C'],\n                data_params['frac_ext_reg']\n                ),\n            'w' : data_params['amplitude_fixed'],\n            },\n        }\n\n# |%%--%%| <NQycjuSucJ|rZFkp6Bip5>\n\nfigure, subplots = plt.subplots(1, 2)\nsubplots[0].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_B']),\n        data['sequential'])\nsubplots[0].set_title(\"Sequential driver inputted in B\")\n\nsubplots[1].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_C']),\n        data['fixed'])\nsubplots[1].set_title(\"Fixed point driver inputted in C\")\n\n# |%%--%%| <rZFkp6Bip5|n7VzpFQee9>\nr\"\"\"°°°\n### Some helpful functions\n°°°\"\"\"\n# |%%--%%| <n7VzpFQee9|p2SoxuC3fL>\n\ndef update_states(data, states, connections, dt, tau, t):\n    for target in states:\n        JR = connections[target][target]['J'].dot(data[target][:, t, np.newaxis])\n        for source in connections[target]:\n            if source != target:\n                JR += connections[target][source]['w'] *\\\n                        connections[target][source]['J'] *\\\n                        data[source][:, t, np.newaxis]\n        states[target] += dt * (-states[target] + JR) / tau\n    return states\n\ndef save_states(data, states, phi, t):\n    for region in states:\n        data[region][:, t, np.newaxis] = phi(states[region])\n    return data\n\n# |%%--%%| <p2SoxuC3fL|TmsjrC0XQK>\n\nfor t in range(n_datapoints):\n    data = save_states(\n            data,\n            states,\n            data_params['activation_function'],\n            t\n            )\n    states = update_states(\n            data,\n            states,\n            connections,\n            data_params['dt'],\n            data_params['tau'],\n            t\n            )\n\nfor i in data:\n    data[i] = data[i] / np.max(data[i])\n\n# |%%--%%| <TmsjrC0XQK|bAuUQSglLX>\n\nfigure, subplots = plt.subplots(1, 3)\nfigure.tight_layout()\nfigure.subplots_adjust(hspace = 0.4, wspace = 0.3)\nsubplots[0].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_A']),\n        data['A']\n        )\n\nsubplots[1].pcolormesh(\n        np.arange(0, data_params['n_neurons_A']),\n        np.arange(0, data_params['n_neurons_A']),\n                  connections['A']['A']['J']\n                  )\n\nfor _ in range(3):\n    idx = npr.randint(0, data_params['n_neurons_A'] - 1)\n    subplots[2].plot(\n            np.arange(0, data_params['sim_time'], data_params['dt']),\n            data['A'][idx, :],\n            )\n\n# |%%--%%| <bAuUQSglLX|47CEdJCjbU>\n\nfigure, subplots = plt.subplots(1, 3)\nfigure.tight_layout()\nfigure.subplots_adjust(hspace = 0.4, wspace = 0.3)\nsubplots[0].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_B']),\n        data['B']\n        )\n\nsubplots[1].pcolormesh(\n        np.arange(0, data_params['n_neurons_B']),\n        np.arange(0, data_params['n_neurons_B']),\n                  connections['B']['B']['J']\n                  )\n\nfor _ in range(3):\n    idx = npr.randint(0, data_params['n_neurons_B'] - 1)\n    subplots[2].plot(\n            np.arange(0, data_params['sim_time'], data_params['dt']),\n            data['B'][idx, :],\n            )\n\n# |%%--%%| <47CEdJCjbU|6ZexEQo9Lz>\n\nfigure, subplots = plt.subplots(1, 3)\nfigure.tight_layout()\nfigure.subplots_adjust(hspace = 0.4, wspace = 0.3)\nsubplots[0].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_C']),\n        data['C']\n        )\n\nsubplots[1].pcolormesh(\n        np.arange(0, data_params['n_neurons_C']),\n        np.arange(0, data_params['n_neurons_C']),\n                  connections['C']['C']['J']\n                  )\n\nfor _ in range(3):\n    idx = npr.randint(0, data_params['n_neurons_C'] - 1)\n    subplots[2].plot(\n            np.arange(0, data_params['sim_time'], data_params['dt']),\n            data['C'][idx, :],\n            )\n\n# |%%--%%| <6ZexEQo9Lz|GyLWs2QXLb>\nr\"\"\"°°°\n# In silico experiment example\n°°°\"\"\"\n# |%%--%%| <GyLWs2QXLb|wr6u9jsZmm>\nr\"\"\"°°°\n## Initialization\n°°°\"\"\"\n# |%%--%%| <wr6u9jsZmm|J8pbXFiyZ0>\n\nn_target_neurons = 300\nlearnList = npr.permutation(n_target_neurons)\ntarget_neurons = learnList[:n_target_neurons]\n\nsim_param = {\n        'n_neurons' : 300,\n        'g' : 1.2,\n        'tau_neuron' : 0.1,\n        'tau_input' : 0.1,\n        'scale_input' : 0.01,\n        'dt' : 0.002,\n        'data_dt' : data_params['dt'],\n        'sim_time' : data_params['sim_time'], #in seconds\n        'learning_rate' : 1.0,\n        'activation_function' : np.tanh,\n        'target_neurons' : target_neurons,\n        }\n\nneurons = init_neurons(sim_param['n_neurons'])\nJ = init_synapses(sim_param['n_neurons'])\nP = init_P(sim_param['n_neurons'], sim_param['learning_rate'])\nH = compute_input(\n        sim_param['n_neurons'],\n        sim_param['tau_input'],\n        sim_param['scale_input'],\n        sim_param['dt'],\n        sim_param['sim_time'])\nexperimental_data = np.concatenate((data['A'], data['B'], data['C']), axis = 0)\nexperimental_data = experimental_data/experimental_data.max()\nexperimental_data = np.minimum(experimental_data, 0.999)\nexperimental_data = np.maximum(experimental_data, -0.999)\nt = 0\n\n# |%%--%%| <J8pbXFiyZ0|z04MsMVEpT>\nr\"\"\"°°°\n## Training\n°°°\"\"\"\n# |%%--%%| <z04MsMVEpT|rFaUgGvzDZ>\n\ndef run(neurons, J, H, P, experimental_data, sim_param, training = True):\n    timestep = 0\n    chi2 = 0\n    simulated_activity = np.zeros((sim_param['n_neurons'], int(sim_param['sim_time'] / sim_param['dt'])))\n    n_timesteps = int(sim_param['sim_time'] / sim_param['dt'])\n    while timestep < n_timesteps:\n        activity = get_activity(neurons, sim_param['activation_function'])\n        simulated_activity[:, timestep, np.newaxis] = activity\n        neurons = update_voltage(neurons,\n                                 sim_param['g'],\n                                 J,\n                                 H[:, timestep, np.newaxis],\n                                 activity,\n                                 sim_param['tau_neuron'],\n                                 sim_param['dt'],\n                                 )\n\n        if training and timestep % int(sim_param['data_dt'] / sim_param['dt']) == 0:\n            data_timestep = int(timestep / (sim_param['data_dt'] / sim_param['dt']))\n            J, P, chi2_temp = train(\n                    activity,\n                    experimental_data[:, data_timestep, np.newaxis],\n                    J,\n                    P,\n                    sim_param['target_neurons'],\n                    )\n            chi2 += chi2_temp\n        timestep += 1\n    return simulated_activity, J, P, chi2\n\n# |%%--%%| <rFaUgGvzDZ|8KJcrkXPhD>\n\ntest, J, P, chi2 = run(neurons, J, H, P, experimental_data, sim_param, training = False)\ndistance = np.linalg.norm(experimental_data - test[:, [i*5 for i in range(experimental_data.shape[1])]])\npvar = 1 - (distance / (math.sqrt(300 * 1200)) * np.std(experimental_data)) ** 2\nprint(chi2, pvar)\n\n# |%%--%%| <8KJcrkXPhD|i7GbUE3XAW>\n\nfigure, subplots = plt.subplots(1, 2)\nsubplots[0].imshow(test, aspect = 'auto')\nsubplots[1].imshow(experimental_data, aspect = 'auto')\n\n# |%%--%%| <i7GbUE3XAW|54L3Vlhdwl>\n\ni = 0\ntest = None\nprev_test = None\nwhile i < 600:\n    test, J, P, chi2 = run(neurons, J, H, P, experimental_data, sim_param)\n    distance = np.linalg.norm(experimental_data - test[:, [i*5 for i in range(experimental_data.shape[1])]])\n    pvar = 1 - (distance / (math.sqrt(300 * 1200)) * np.std(experimental_data)) ** 2\n    print(i, chi2, pvar)\n\n    if i > 500 or i % 100 == 0:\n        figure, subplots = plt.subplots(1, 2)\n        subplots[0].imshow(test, aspect = 'auto')\n        subplots[1].imshow(experimental_data, aspect = 'auto')\n        plt.show()\n    i += 1", "cmd_opts": " -s --md_cell_start=r\\\"\\\"\\\"°°°", "import_complete": 1, "terminal": "nvimterm"}