{"cmd": "r\"\"\"°°°\n# Recurrent spiking neural networks\n\nThis notebook provides a computational pipeline to:\n* Set up a recurrent-spiking neural network.\n* Generate data to give to it as input.\n* Train the network so that it matches the data.\n* Provide some examples on what type of analysis can be done on the trained network.\n\nThis notebook has been built following:\n* [The CURBD tool](https://github.com/rajanlab/CURBD)\n* The notebooks provided by Rajan Lab at this [link](https://www.rajanlab.com/cosyne2021)\n°°°\"\"\"\n#|%%--%%| <NEH5SUEzp5|J1RTFC9q80>\nr\"\"\"°°°\n# Importing libraries\n°°°\"\"\"\n#|%%--%%| <J1RTFC9q80|AZwMTYFGUW>\n\nimport numpy as np #Vector and matrix operations\nimport matplotlib.pyplot as plt #Plotting\nimport numpy.random as npr #Random number generation\nimport math #Math operations\nimport scipy as sp #Scientific computing\nfrom sklearn.decomposition import PCA #Principal component analysis methods\n\n# |%%--%%| <AZwMTYFGUW|QhYxSMc5Z6>\nr\"\"\"°°°\n# Creating the network\n°°°\"\"\"\n# |%%--%%| <QhYxSMc5Z6|L7xcJxIvJC>\nr\"\"\"°°°\n## Neurons\nNeurons are modeled by the time evolution of their membrane potential $V$:\n\n$$\\tau\\frac{dV_{i}}{dt} = -V + g\\sum\\limits_{j}J_{ij}\\phi(V_{j}) + I_{i}$$\n\nThis differential equation is solved using the Euler method, so the evolution of the voltage from one timestep to another is:\n\n$$V_{i}(t + \\Delta t) = V_{i}(t) + \\frac{-V_{i}(t) + g\\sum\\limits_{j}J_{ij}\\phi(V_{j}(t)) + I_{i}(t)}{\\tau}\\Delta t$$\n\nWhere:\n\n* $\\phi(V)$ is a non-linear activation function, representing the firing rate of the neuron.\n* $J_{ij}$ is the strenght of the synapses going from neuron $j$ to neuron $i$.\n* g is the conductance of input synapses. It is chosen to be large enough as to favour complex dynamics (usually $>1$).\n* $\\tau$ is the time constant of the neuron.\n°°°\"\"\"\n# |%%--%%| <L7xcJxIvJC|TpmXKvZIwD>\n\nn_neurons = 300\ng = 1.5 #Neuron's conductance\ntau_neuron = 0.1 #Neuron's time constant\n\n#The neurons in the network are represented by a vector containing the membrane\n#potential value for each one of them.\ndef init_neurons(n_neurons):\n    return npr.normal(\n            -0.7, #Minimum value\n            0.7, #Maximum value\n            (n_neurons, 1) #(n_neurons, n_timesteps)\n            )\n\ninit_neurons(n_neurons)\n\n# |%%--%%| <TpmXKvZIwD|jahVKoLM65>\n\n#The activity, or firing rate is the output of the non-linear activation\n#function applied to the membrane potential.\ndef get_activity(V, phi):\n    return phi(V)\n\n#The voltage is updated with Euler's method\ndef update_voltage(V, g, J, I, activity, tau, dt):\n    return V + (-V + g * J.dot(activity) + I) * dt / tau\n\n# |%%--%%| <jahVKoLM65|xN8JsfYrEi>\nr\"\"\"°°°\n## Synapses\nThe synapses are represented as a matrix $J$ of size $n \\times n$. where $n$ is the number of neurons.\nThe value of $J_{ij}$ represents the strength of the connection between neuron $i$ and neuron $j$, such that:\n* $J_{ij} > 0$ means that neuron $i$ excites neuron $j$.\n* $J_{ij} < 0$ means that neuron $i$ inhibits neuron $j$.\n* $J_{ij} = 0$ means that there is no connection between neuron $i$ and neuron $j$.\n\nThe matrix is initialized with random values distributed as a normal distribution and then normalized according to the number of neurons $n$:\n\n$$J_{ij} \\sim \\frac{N(0, 1)}{\\sqrt{n}}$$\n°°°\"\"\"\n# |%%--%%| <xN8JsfYrEi|tZpgGDHTAA>\n\n#The synapses in the network are initialized as a random square matrix,\n#where rows represent the source neuron and the columns the target\ndef init_synapses(n_neurons):\n    return npr.randn(n_neurons, n_neurons) / math.sqrt(n_neurons)\ninit_synapses(n_neurons)\n\n# |%%--%%| <tZpgGDHTAA|DHjIt9fgCu>\n\n#Plotting the synapse's matrix\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\nsubplot.imshow(init_synapses(n_neurons))\n\n# |%%--%%| <DHjIt9fgCu|N2x7xaeO4F>\nr\"\"\"°°°\n## Input\nThe input I is computed as:\n\n$$I_{i}(t) = \\eta_{i}(t) + (I_{i}(t - \\Delta t) - \\eta_{i}(t))e^{-\\frac{\\Delta t}{\\tau}}$$\n\nWhere $\\eta_{i}(t)$ is a white noise process with a time constant $\\tau$.\n\nThis is done to ensure that the input of different neurons is not correlated.\n°°°\"\"\"\n# |%%--%%| <N2x7xaeO4F|Qcda8uk2iM>\n\ntau_input = 0.1 #The time constant of the input\ndt = 0.01 / 5 #The timestep of the input\nscale_input = 0.01 #The scale of the input\n\ndef compute_input(n_neurons, tau_input, scale_input, dt, sim_time):\n    n_timesteps = int(np.ceil(sim_time / dt)) #The number of timesteps\n    eta = npr.randn(n_neurons, n_timesteps) * math.sqrt(tau_input / dt) #The noise process\n    I = np.ones((n_neurons, n_timesteps)) #Initializing the input\n\n    for i in range(1, n_timesteps): #Computing the input\n        I[:, i] = eta[:, i] + (I[:, i - 1] - eta[:, i]) * np.exp(-dt / tau_input)\n\n    return I * scale_input\n\nI = compute_input(300, tau_input, scale_input, dt, 12)\n\n# |%%--%%| <Qcda8uk2iM|c0YKNrAEh4>\n\n#Plotting the input coming to a single neuron\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\n\nsubplot.plot(I[0, :])\nsubplot.set_xlabel(\"Time steps\")\nsubplot.set_ylabel(\"Input current\")\n\n# |%%--%%| <c0YKNrAEh4|FwK2GquHuk>\n\n#Plotting the input coming to all the neurons\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\n\nsubplot.imshow(I, aspect = 'auto')\nsubplot.set_xlabel(\"Time steps\")\nsubplot.set_ylabel(\"Neurons\")\n\n# |%%--%%| <FwK2GquHuk|rkJqlKN2rh>\nr\"\"\"°°°\n## Activation function\nThe activation function is any non-linear function like:\n* The sigmoid.\n* The hyperbolic tangent.\n* The rectified linear unit (ReLU)\n\nIn this case the hyperbolic tangent is used:\n\n$$\\phi(x) = \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n\nWith its derivative:\n\n$$\\phi'(x) = 1 - \\tanh^{2}(x) = 1 - \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}^{2}$$\n°°°\"\"\"\n# |%%--%%| <rkJqlKN2rh|L1DW3X5I98>\n\nphi = np.tanh\n\n# |%%--%%| <L1DW3X5I98|fcbTSOnoRZ>\n\n#Plotting the activation function\nx = np.linspace(-5, 5, 1000)\ny = phi(x)\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\nsubplot.spines['left'].set_position('center')\nsubplot.spines['bottom'].set_position('center')\nsubplot.spines['right'].set_color('none')\nsubplot.spines['top'].set_color('none')\nsubplot.xaxis.set_ticks_position('bottom')\nsubplot.yaxis.set_ticks_position('left')\nsubplot.set_title(\"Activation function\")\nsubplot.plot(x, y)\n\n# |%%--%%| <fcbTSOnoRZ|nsVBtXprrj>\n\n#Plotting the activation function's derivative\nfigure, subplot = plt.subplots(1, figsize=(10, 5))\nsubplot.spines['left'].set_position('center')\nsubplot.spines['bottom'].set_position('center')\nsubplot.spines['right'].set_color('none')\nsubplot.spines['top'].set_color('none')\nsubplot.xaxis.set_ticks_position('bottom')\nsubplot.yaxis.set_ticks_position('left')\nsubplot.set_title(\"Derivative of the activation function\")\nsubplot.plot(x, 1 - y**2)\n\n# |%%--%%| <nsVBtXprrj|vzC7GmU5JD>\nr\"\"\"°°°\n# Training\nTraining consists of updating the strength of the synapses connections $J_{ij}$ so that the activity of the neurons is as close as possible to the one found in the experimental data.\nThis is done by computing the error between the experimental data and the output of the network and then updating the synapses connections according to it:\n\n$$J_{ij}(t) = J_{ij}(t-1) + \\Delta J_{ij}(t)$$\n\n$$\\Delta J_{ij}(t) = c\\cdot e_{i}(t)\\sum\\limits_{k=1}^N P_{jk}(t)\\phi_{k}(t)$$\n°°°\"\"\"\n# |%%--%%| <vzC7GmU5JD|MQgpY2zxrQ>\nr\"\"\"°°°\n## Error function\nThe error function is the difference between the experimental data and the output of the network:\n\n$$e_{i}(t) = z_{i}(t) - a_{i}(t)$$\n°°°\"\"\"\n# |%%--%%| <MQgpY2zxrQ|OHXncPsWTH>\n\ndef get_error(simulated, experimental):\n    return simulated - experimental\n\n# |%%--%%| <OHXncPsWTH|FWsNOtQM3m>\nr\"\"\"°°°\n## Inverse cross-correlation matrix\n$P$ is the inverse cross-correlation matrix of the firing rate of two neurons in the network:\n\n$$P_{ij} = \\langle\\phi_{i}\\phi_{j}\\rangle^{-1}$$\n\nThis is costly to compute at every timestep, so it is computed iteratively as:\n\n$$P(t) = P(t-1) - c\\cdot ||(P(t-1)\\cdot\\phi(t))|| = P(t-1) - c\\cdot((P(t-1)\\cdot\\phi(t))(P(t-1)\\cdot\\phi(t))^T)$$\n\nWith:\n\n$$P(0) = \\lambda\\cdot I$$\n\nWhere $\\lambda$ is the learning rate\n°°°\"\"\"\n# |%%--%%| <FWsNOtQM3m|wwvyiUURv6>\n\nlearning_rate = 1.0 #Works best when set to be 1 to 10 times the overall amplitude of the external inputs\n\n#Initializing the inverse cross-correlation matrix\ndef init_P(n_neurons, learning_rate):\n    return learning_rate * np.eye(n_neurons)\n\n# |%%--%%| <wwvyiUURv6|qWKW82G6gz>\n\n#Computing the dot product of the inverse cross-correlation\n#matrix and the activity of the neurons, this is a helper\n#function that will be used in a number of methods.\ndef get_activity_P(P, activity):\n    return P.dot(activity)\n\n# |%%--%%| <qWKW82G6gz|ERuwWeRy9a>\n\n#Defining the iterative computation of P\ndef update_P(P, activity_P, c):\n    return P - c * activity_P.dot(activity_P.T)\n\n# |%%--%%| <ERuwWeRy9a|TLRdRxgdiJ>\nr\"\"\"°°°\n## Scaling term\nThe scaling term $c$ scales the update of the synapses connections.\nIt is computed as:\n\n$c = \\frac{1}{1+\\phi^{T}(t)\\cdot(P(t-1)\\cdot\\phi(t))}$\n°°°\"\"\"\n# |%%--%%| <TLRdRxgdiJ|h78ov2I9Ex>\n\ndef get_c(activity_P, activity):\n    aPa = activity.T.dot(activity_P).item() # The result is a singleton, item extracts the number\n    return 1 / (1 + aPa)\n\n# |%%--%%| <h78ov2I9Ex|NTAIKNIRyt>\nr\"\"\"°°°\n## Update of the synapses Matrix\nRecall:\n\n$$\\Delta J_{ij}(t) = c\\cdot e_{i}(t)\\sum\\limits_{k=1}^N P_{jk}(t)\\phi_{k}(t)$$\n°°°\"\"\"\n# |%%--%%| <NTAIKNIRyt|4aDnFrdZTl>\n\n#target_neurons is a list of the indices of neurons targeted by\n#learning. This is necessary as the experimental data could contain the\n#firing rates of only a subset of neurons in the network.\ndef update_J(J, activity_P, error, c, target_neurons):\n    return J[:, target_neurons] - c * np.outer(error.flatten(), activity_P.flatten())\n\n# |%%--%%| <4aDnFrdZTl|7QIDJja5kb>\nr\"\"\"°°°\n## Training function\nNow that all the function have been defined, they can be collected in a single function that performs the training of the network.\n°°°\"\"\"\n# |%%--%%| <7QIDJja5kb|by855Yt3K7>\n\ndef train(simulated, experimental, J, P, target_neurons):\n    error = get_error(simulated, experimental)\n    activity_P = get_activity_P(P, simulated[target_neurons,:])\n    c = get_c(activity_P, simulated[target_neurons,:])\n    P = update_P(P, activity_P, c)\n    J[:, target_neurons.flatten()] = update_J(J, activity_P, error, c, target_neurons)\n    return J, P, np.mean(error ** 2)\n\n# |%%--%%| <by855Yt3K7|agBdu0TvNR>\nr\"\"\"°°°\n# The experimental data\nTo demonstrate the capabilities of the network the experimental data will be generated in silico as well.\nThe data will be generated according to a three region ground truth model.\nEach of the tree regions will contain $100$ neurons and will be driven by a different type of input:\n\n* The first region (A) will be driven by only by the recurrent inputs from the other two regions.\n* The second region (B) will be driven by a network generating a Gaussian \"bump\" $Q_{i}(t)$ propagating agross the network:\n* The final region (C) will be driven by a network generating a fixed point that is shifted then to another during the generation.\nThe fixed points are generated by sampling $SQ_{i}(t)$ at two differend time points and holding them at the sampled value for the duration of the fixed point.\n\nExternal inputs are connected to half of the units in their respective regions with a fixed negative weight (inhibitory) for region B and positive (excitatory) for region C.\n°°°\"\"\"\n# |%%--%%| <agBdu0TvNR|2LNjZRuYSx>\nr\"\"\"°°°\n## Generating the external inputs\n°°°\"\"\"\n# |%%--%%| <2LNjZRuYSx|Couplet>\nr\"\"\"°°°\n### The Gaussian bump\nThe Gaussian bump is computed as:\n\n$$SQ_{i}(t) = e^{-\\frac{1}{2\\sigma^2}\\left(\\frac{1-\\sigma-Nt}{T}\\right)^2}$$\n\nWhere:\n* $\\sigma$ is the width of the bump across the population.\n* $N$ the population size.\n* $t$ the total simulation time.\n°°°\"\"\"\n# |%%--%%| <Couplet|vF7CwS320T>\n\n#Cutoff_translation is used to compute when the bump stops moving between neurons\n#and starts to remain fixed\ndef compute_x_bump(sim_time, dt, n_neurons, width, cutoff_traslation):\n    \"\"\"Selects which neurons have a higher firing rate.\n    They will be sampled from a Gaussian distrubtion\n    \"\"\"\n    n_samples = int(math.ceil(sim_time / dt))\n    t_data = np.arange(0, sim_time, dt)\n    x_bump = np.zeros((n_neurons, n_samples))\n    n_width = width * n_neurons #from percentage to number\n    norm = 2*n_width **2\n    cutoff = math.ceil(n_samples / 2) + cutoff_traslation\n\n    for i in range(n_neurons):\n        x_bump[i, :] = np.exp(- ((i - n_width - n_neurons * t_data / (t_data[-1] / 2)) **2 / norm))\n        x_bump[i, cutoff:] = x_bump[i, cutoff]\n\n    return x_bump\n\n# |%%--%%| <vF7CwS320T|zoK9rq0cf4>\nr\"\"\"°°°\n### Sequence driving network\nThe sequence driving network takes the Gaussian bump and shifts it across neruons in a sequential manner until the cutoff.\n°°°\"\"\"\n# |%%--%%| <zoK9rq0cf4|d9NCDdW6KB>\n\n#The lead time is a \"resting time\" put at the beginning of the current,\n#Where it remains constant.\ndef sequence_driving_network(x_bump, dt , lead_time):\n    hbump = np.log((x_bump + 0.01)/(1-x_bump+0.01))\n    #Shift to zero\n    hbump = hbump - np.min(hbump)\n    #Normalization\n    hbump = hbump / np.max(hbump)\n    #Add lead time\n    newmat = np.tile(hbump[:, 1, np.newaxis], (1, math.ceil(lead_time / dt)))\n    hbump = np.concatenate((newmat, hbump), axis=1)\n    return hbump\n\n# |%%--%%| <d9NCDdW6KB|UK5W1tRRRH>\n\n#Plotting an example of a sequence driving network.\nx_bump = compute_x_bump(10, 0.01, 100, 0.1, -10)\ntest = sequence_driving_network(x_bump, 0.01, 2)\nfigure, subplot = plt.subplots(1)\nplt.pcolormesh(np.arange(0, 12, 0.01), np.arange(0, 100), test)\n\n# |%%--%%| <UK5W1tRRRH|gy9STCGxnN>\nr\"\"\"°°°\n### Fixed point driving network\nThe fixed point driving network generates a fixed point that it is kept fixed until a certain point during the simulation, where it is traslated to another neuron.\n°°°\"\"\"\n# |%%--%%| <gy9STCGxnN|Mi4GDataJz>\n\n#Generates a fixed point that is shifted during the simulation,\n#cutoff_translation: where the shift happen\n#before_shift where to pick the current from x_bump before the shift\n#after_shift where to pick the current from x_bump after the shift\ndef fixed_point_driver(xbump, sim_time, lead_time, dt, n_neurons, cutoff_traslation, before_shift, after_shift):\n    n_samples = int(math.ceil((sim_time - lead_time) / dt))\n    x_fp = np.zeros(xbump.shape)\n    cutoff = math.ceil(n_samples / 2) + cutoff_traslation\n    for i in range(n_neurons):\n        front = xbump[i, before_shift] * np.ones((1, cutoff))\n        back = xbump[i, after_shift] * np.ones((1, n_samples - cutoff))\n        x_fp[i, :] = np.concatenate((front, back), axis = 1)\n    y_fp = np.log((x_fp + 0.01)/(1-x_fp+0.01))\n    #Shift to zero\n    y_fp -= np.min(y_fp)\n    #Normalization\n    y_fp = y_fp / np.max(y_fp)\n    #Add leading time\n    newmat = np.tile(y_fp[:, 1, np.newaxis], (1, math.ceil(lead_time/dt)))\n    y_fp = np.concatenate((newmat, y_fp), axis=1)\n    return y_fp\n\n\n# |%%--%%| <Mi4GDataJz|XSIfrLRk46>\n\n#Plot of an example of fixed point driver\ntest = fixed_point_driver(x_bump, 12, 2, 0.01, 100, -10, 10, 300)\nfigure, subplot = plt.subplots(1)\nplt.pcolormesh(np.arange(0, 12, 0.01), np.arange(0, 100), test)\n\n# |%%--%%| <XSIfrLRk46|ns2PdFjA1O>\nr\"\"\"°°°\n## Connectivity\n°°°\"\"\"\n# |%%--%%| <ns2PdFjA1O|Qbdg5h3HF7>\nr\"\"\"°°°\n### Intra-region connectivity\nEach of the three regions will have random inter-connections.\nThe strength of the connection is sampled by a normalized normal distribution:\n\n$$J_{ij} \\sim \\mathcal{N}(0, \\frac{1}{\\sqrt{N_{in}}})$$\n\nAnd then scaled by a chaos factor $g$:\n\n$$J_{ij} = g\\cdot J_{ij}$$\n\n$g$ has to be sufficiently large so to facilitate chaotic dynamics in the network.\n°°°\"\"\"\n# |%%--%%| <Qbdg5h3HF7|xlXbPekh9D>\n\ndef intra_region_connectivity(n_neurons, g):\n    J = npr.randn(n_neurons, n_neurons)\n    J = (g / np.sqrt(n_neurons)) * J\n    return J\n\n# |%%--%%| <xlXbPekh9D|Q0M8UfFt7V>\n\n#Plotting some example of intra-region connectivity\nJ_a = intra_region_connectivity(100, 1.8)\nJ_b = intra_region_connectivity(100, 1.5)\nJ_c = intra_region_connectivity(100, 1.5)\n\nfigure, subplot = plt.subplots(1, 3)\nsubplot[0].imshow(J_a, aspect = 'auto')\nsubplot[0].set_title(\"RNN-A synaptic strenght\")\nsubplot[1].imshow(J_b, aspect = 'auto')\nsubplot[1].set_title(\"RNN-B synaptic strenght\")\nsubplot[2].imshow(J_c, aspect = 'auto')\nsubplot[2].set_title(\"RNN-C synaptic strenght\")\n\n# |%%--%%| <Q0M8UfFt7V|XC1z8bZenO>\nr\"\"\"°°°\n### Inter-region connectivity\nThe number of connection is controlled by the `frac_inter_reg`, which controls the percentage of connected neurons\n°°°\"\"\"\n# |%%--%%| <XC1z8bZenO|R9NUMkejtt>\n\n\n#Returns the array source_neurons of the neurons\n#in the source population, where source_neurons[i] = 1 if\n#neuron i is connected to the population and 0 otherwise\ndef source_region_neurons_connected(n_neurons, connected_fraction):\n    n_connected = int(n_neurons * connected_fraction)\n    source_neurons = np.zeros((n_neurons, 1))\n    random_connected_neurons = npr.permutation(n_neurons)\n    source_neurons[random_connected_neurons[0:n_connected]] = 1\n    return source_neurons\n\n# |%%--%%| <R9NUMkejtt|h3uOj5FdsB>\n\nfrac_inter_reg = 0.05\nconnection_A_to_B = source_region_neurons_connected(100, frac_inter_reg)\n\nfrac_external_reg = 0.5\n\nconnection_sequence_to_B = source_region_neurons_connected(100, frac_external_reg)\n\nprint(\"The number of connection between A and B is: \", np.sum(connection_A_to_B))\nprint(\"The number of connection between the sequence driver and B is: \", np.sum(connection_sequence_to_B))\n\n# |%%--%%| <h3uOj5FdsB|tl4zcmyqGm>\nr\"\"\"°°°\n## Initial state\nThe initial state is a `n_neurons` vector of values sampled from a normal distribution.\n°°°\"\"\"\n# |%%--%%| <tl4zcmyqGm|WjuzEEzOvb>\n\ndef get_initial_state(n_neurons):\n    return 2 * npr.rand(n_neurons, 1) - 1\n\n# |%%--%%| <WjuzEEzOvb|7WRHXhPqwm>\nr\"\"\"°°°\n## Generating the data\nTo generate the data a number of parameters is necessary, so they are all collected in a dictionary.\n°°°\"\"\"\n# |%%--%%| <7WRHXhPqwm|teucre>\n\ndata_params = {\n        'sim_time' : 12,\n        'lead_time' : 2,\n        'dt' : 0.01,\n        'bump_width' : 0.2,\n        'n_neurons_A' : 100,\n        'n_neurons_B' : 100,\n        'n_neurons_C' : 100,\n        'g_A' : 1.8,\n        'g_B' : 1.5,\n        'g_C' : 1.5,\n        'frac_inter_reg' : 0.05,\n        'frac_ext_reg' : 0.5,\n        'activation_function' : np.tanh,\n        'tau' : 0.1,\n        'amplitude_inter_region' : 0.02,\n        'amplitude_sequential' : 1,\n        'amplitude_fixed' : -1,\n        'cutoff_traslation' : 100,\n        'before_shift' : 10,\n        'after_shift' : 300,\n        }\n\nn_datapoints = int(data_params['sim_time'] / data_params['dt'])\ndata = {}\nconnections = {}\n\n# |%%--%%| <teucre|lVJAoZSZ3Z>\nr\"\"\"°°°\n### Setting up all the necessary variables\n°°°\"\"\"\n# |%%--%%| <lVJAoZSZ3Z|NQycjuSucJ>\n\nx_bump = compute_x_bump(\n        data_params['sim_time'] - data_params['lead_time'],\n        data_params['dt'],\n        data_params['n_neurons_B'],\n        data_params['bump_width'],\n        -data_params['cutoff_traslation'],\n        )\n\n#Will contain the experimental data that will be fed into\n#the training algorithm\ndata = {\n        'A' : np.empty((data_params['n_neurons_A'], n_datapoints)),\n        'B' : np.empty((data_params['n_neurons_B'], n_datapoints)),\n        'C' : np.empty((data_params['n_neurons_C'], n_datapoints)),\n        'sequential' : sequence_driving_network(\n            x_bump,\n            data_params['dt'],\n            data_params['lead_time']\n            ),\n        'fixed' : fixed_point_driver(\n            x_bump,\n            data_params['sim_time'],\n            data_params['lead_time'],\n            data_params['dt'],\n            data_params['n_neurons_C'],\n            data_params['cutoff_traslation'],\n            data_params['before_shift'],\n            data_params['after_shift'],\n            ),\n        }\n\ndata['A'][:] = np.NaN\ndata['B'][:] = np.NaN\ndata['C'][:] = np.NaN\n\n#The states of the neurons in each regions.\nstates = {\n        'A' : get_initial_state(data_params['n_neurons_A']),\n        'B' : get_initial_state(data_params['n_neurons_B']),\n        'C' : get_initial_state(data_params['n_neurons_C']),\n        }\n#The connection between regions.\nconnections['A'] = {\n        'A' : {\n            'J' : intra_region_connectivity(\n                data_params['n_neurons_A'],\n                data_params['g_A']\n                ),\n            },\n        'B' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_A'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'C' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_A'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        }\nconnections['B'] = {\n        'A' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_B'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'B' : {\n            'J' : intra_region_connectivity(\n                data_params['n_neurons_B'],\n                data_params['g_B']\n                ),\n            },\n        'C' : {\n            'region' : 'C',\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_B'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'sequential' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_B'],\n                data_params['frac_ext_reg']\n                ),\n            'w' : data_params['amplitude_sequential'],\n            },\n        }\nconnections['C'] = {\n        'A' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_C'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'B' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_C'],\n                data_params['frac_inter_reg']\n                ),\n            'w' : data_params['amplitude_inter_region'],\n            },\n        'C' : {\n            'J' : intra_region_connectivity(\n                data_params['n_neurons_C'],\n                data_params['g_C']\n                ),\n        },\n        'fixed' : {\n            'J' : source_region_neurons_connected(\n                data_params['n_neurons_C'],\n                data_params['frac_ext_reg']\n                ),\n            'w' : data_params['amplitude_fixed'],\n            },\n        }\n\n# |%%--%%| <NQycjuSucJ|rZFkp6Bip5>\n\n#Plotting the input currents\nfigure, subplots = plt.subplots(1, 2)\nsubplots[0].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_B']),\n        data['sequential'])\nsubplots[0].set_title(\"Sequential driver inputted in B\")\n\nsubplots[1].pcolormesh(\n        np.arange(0, data_params['sim_time'], data_params['dt']),\n        np.arange(0, data_params['n_neurons_C']),\n        data['fixed'])\nsubplots[1].set_title(\"Fixed point driver inputted in C\")\n\n# |%%--%%| <rZFkp6Bip5|n7VzpFQee9>\nr\"\"\"°°°\n### Updating the neurons and saving the data\nHere two functions are defined:\n* `save_states` saves the states of the neurons in the data dictionary.\n* `update_states` updates the states of the neurons in the states dictionary.\n\nNeurons are updated according to the equation:\n\n$$r_{region, i}(t) = r_{region, i}(t - \\Delta t) + \\frac{\\Delta t}{\\tau} \\left( -r_{region, i}(t - \\Delta t) + \\Delta r_{region, i}(t) \\right)$$\n\nWhere:\n\n$$\\Delta r_{region, i}(t) = g_{region}J_{region}r_{region, i}(t-\\Delta t) + \\sum_{region'} w_{region, region'}J_{region, region'}r_{region', i}(t-\\Delta t)$$\n°°°\"\"\"\n# |%%--%%| <n7VzpFQee9|p2SoxuC3fL>\n\ndef update_states(data, states, connections, dt, tau, t):\n    for target in states:\n        #The weight for intra-region connection is already included in J\n        JR = connections[target][target]['J'].dot(data[target][:, t, np.newaxis])\n        for source in connections[target]:\n            if source != target:\n                JR += connections[target][source]['w'] *\\\n                        connections[target][source]['J'] *\\\n                        data[source][:, t, np.newaxis]\n        states[target] += dt * (-states[target] + JR) / tau\n    return states\n\ndef save_states(data, states, phi, t):\n    for region in states:\n        data[region][:, t, np.newaxis] = phi(states[region])\n    return data\n\n# |%%--%%| <p2SoxuC3fL|TmsjrC0XQK>\n\nfor t in range(n_datapoints):\n    data = save_states(\n            data,\n            states,\n            data_params['activation_function'],\n            t\n            )\n    states = update_states(\n            data,\n            states,\n            connections,\n            data_params['dt'],\n            data_params['tau'],\n            t\n            )\n\n#Data normalization\nfor i in data:\n    data[i] = data[i] / np.max(data[i])\n\n#Concatenating the data in one big network\nexperimental_data = np.concatenate((data['A'], data['B'], data['C']), axis = 0)\n#Normalization\nexperimental_data = experimental_data/experimental_data.max()\n#Clipping to a maximum and minimum value\nexperimental_data = np.minimum(experimental_data, 0.999)\nexperimental_data = np.maximum(experimental_data, -0.999)\n\n#|%%--%%| <TmsjrC0XQK|5Y2cgD9MYR>\nr\"\"\"°°°\n### Plotting the obtained experimental data\n°°°\"\"\"\n#|%%--%%| <5Y2cgD9MYR|wJhF1C1F1c>\n\ndef plot_region(data_params, data, region):\n    figure, subplots = plt.subplots(1, 3)\n    figure.tight_layout()\n    figure.subplots_adjust(hspace = 0.4, wspace = 0.3)\n    subplots[0].pcolormesh(\n            np.arange(0, data_params['sim_time'], data_params['dt']),\n            np.arange(0, data_params[f'n_neurons_{region}']),\n            data[region]\n            )\n\n    subplots[1].pcolormesh(\n            np.arange(0, data_params['n_neurons_A']),\n            np.arange(0, data_params['n_neurons_A']),\n                      connections[region][region]['J']\n                      )\n\n    #Some example neurons\n    for _ in range(3):\n        idx = npr.randint(0, data_params['n_neurons_A'] - 1)\n        subplots[2].plot(\n                np.arange(0, data_params['sim_time'], data_params['dt']),\n                data[region][idx, :],\n                )\n\n# |%%--%%| <wJhF1C1F1c|bAuUQSglLX>\n\n#Plotting the resulting data for region A\nplot_region(data_params, data, 'A')\n\n# |%%--%%| <bAuUQSglLX|47CEdJCjbU>\n\n#Plotting the resulting data for region B\nplot_region(data_params, data, 'B')\n\n# |%%--%%| <47CEdJCjbU|6ZexEQo9Lz>\n\n#Plotting the resulting data for region C\nplot_region(data_params, data, 'C')\n\n# |%%--%%| <6ZexEQo9Lz|GyLWs2QXLb>\nr\"\"\"°°°\n# In silico experiment example\nThe data obtained in the previous section will be used in the pipeline defined at the beginning of this notebook to train a recurrent neural network.\nDuring the simulation two values will be recorded:\n\n* `chi2` is the value of the chi-squared error between the output of the network and the experimental data.\n* `pvar` is the value of the percentage of variance explained by the network.\n\nThe network will be trained to minimize the chi-squared error and maximize the percentage of variance explained.\n°°°\"\"\"\n# |%%--%%| <GyLWs2QXLb|wr6u9jsZmm>\nr\"\"\"°°°\n## Initialization\nGenerating the network and the initial states.\n°°°\"\"\"\n# |%%--%%| <wr6u9jsZmm|J8pbXFiyZ0>\n\nn_target_neurons = 300\n#Randomizing the indices makes the network more robust\n#and converge faster\nlearn_list = npr.permutation(n_target_neurons)\ntarget_neurons = learn_list[:n_target_neurons]\n\nsim_param = {\n        'n_neurons' : 300,\n        'g' : 1.2,\n        'tau_neuron' : 0.1,\n        'tau_input' : 0.1,\n        'scale_input' : 0.01,\n        'dt' : 0.002,\n        'data_dt' : data_params['dt'],\n        'sim_time' : data_params['sim_time'], #in seconds\n        'learning_rate' : 1.0,\n        'activation_function' : np.tanh,\n        'target_neurons' : target_neurons,\n        }\n\nneurons = init_neurons(sim_param['n_neurons'])\nJ = init_synapses(sim_param['n_neurons'])\nP = init_P(sim_param['n_neurons'], sim_param['learning_rate'])\nH = compute_input(\n        sim_param['n_neurons'],\n        sim_param['tau_input'],\n        sim_param['scale_input'],\n        sim_param['dt'],\n        sim_param['sim_time'])\n\n# |%%--%%| <J8pbXFiyZ0|z04MsMVEpT>\nr\"\"\"°°°\n## Training\nTo train the network a single run will have to be repeated a number of times.\nDuring the run the network is left to evolve alone until it finds a data point in the experimental data.\nAt that point it will update the values of its synaptic connectivity $J$ to minimize the error with the experimental data\n°°°\"\"\"\n# |%%--%%| <z04MsMVEpT|rFaUgGvzDZ>\n\ndef run(neurons, J, H, P, experimental_data, sim_param, training = True):\n    timestep = 0\n    chi2 = 0\n    simulated_activity = np.zeros((sim_param['n_neurons'], int(sim_param['sim_time'] / sim_param['dt'])))\n    n_timesteps = int(sim_param['sim_time'] / sim_param['dt'])\n    while timestep < n_timesteps:\n        activity = get_activity(neurons, sim_param['activation_function'])\n        simulated_activity[:, timestep, np.newaxis] = activity\n        neurons = update_voltage(neurons,\n                                 sim_param['g'],\n                                 J,\n                                 H[:, timestep, np.newaxis],\n                                 activity,\n                                 sim_param['tau_neuron'],\n                                 sim_param['dt'],\n                                 )\n\n        if training and timestep % int(sim_param['data_dt'] / sim_param['dt']) == 0:\n            data_timestep = int(timestep / (sim_param['data_dt'] / sim_param['dt']))\n            J, P, chi2_temp = train(\n                    activity,\n                    experimental_data[:, data_timestep, np.newaxis],\n                    J,\n                    P,\n                    sim_param['target_neurons'],\n                    )\n            chi2 += chi2_temp\n        timestep += 1\n    return simulated_activity, J, P, chi2\n\n# |%%--%%| <rFaUgGvzDZ|8KJcrkXPhD>\n\n#An example of a network run with training disabled (no updating of J)\ntest, J, P, chi2 = run(neurons, J, H, P, experimental_data, sim_param, training = False)\ndistance = np.linalg.norm(experimental_data - test[:, [i*5 for i in range(experimental_data.shape[1])]])\npvar = 1 - (distance / (math.sqrt(300 * 1200)) * np.std(experimental_data)) ** 2\nprint(chi2, pvar)\n\n# |%%--%%| <8KJcrkXPhD|i7GbUE3XAW>\n\nfigure, subplots = plt.subplots(1, 3)\nsubplots[0].imshow(test, aspect = 'auto')\nsubplots[1].imshow(experimental_data, aspect = 'auto')\nrandom_neuron_index = npr.choice(target_neurons)\nsubplots[2].plot(\n        np.arange(0, sim_param['sim_time'], sim_param['data_dt']),\n        experimental_data[random_neuron_index, :],\n        label = 'experimental'\n        )\nsubplots[2].plot(\n        np.arange(0, sim_param['sim_time'], sim_param['dt']),\n        test[random_neuron_index, :],\n        label = 'simulated'\n        )\n\n# |%%--%%| <i7GbUE3XAW|54L3Vlhdwl>\n\ndef plot_train_evolution(figure, subplots, chi2s, pvars, neuron_index, state, experimental_data, plots):\n    if not figure:\n        plt.ion()\n        figure, subplots = plt.subplots(2, 3)\n        plots['exp_neuron'], = subplots[0, 2].plot(\n                np.arange(0, sim_param['sim_time'], sim_param['data_dt']),\n                experimental_data[neuron_index, :],\n                label = 'experimental'\n                )\n        plots['sim_neuron'], = subplots[0, 2].plot(\n                np.arange(0, sim_param['sim_time'], sim_param['dt']),\n                state[neuron_index, :],\n                label = 'simulated',\n                )\n        subplots[0,2].legend()\n        subplots[0,2].set_ylim(-1,1)\n        plots['state'] = subplots[0, 0].imshow(state, aspect = 'auto')\n        subplots[0, 1].imshow(experimental_data, aspect = 'auto')\n        plots['chi2'], = subplots[1, 0].plot(chi2s)\n        plots['pvars'], = subplots[1, 1].plot(pvars)\n        subplots[1, 2].set_visible(False)\n        subplots[1, 0].set_position([0.24,0.125,0.228,0.343])\n        subplots[1, 1].set_position([0.55,0.125,0.228,0.343])\n\n    subplots[0,0].set_title(\"Simulated neurons activation\")\n    subplots[0,1].set_title(\"Experimental neurons activation\")\n    subplots[0,2].set_title(\"Neuron {}\".format(neuron_index))\n\n    plots['sim_neuron'].set_ydata(state[random_neuron_index, :])\n    plots['exp_neuron'].set_ydata(experimental_data[random_neuron_index, :])\n    plots['state'].set_data(state)\n    subplots[1, 0].plot(chi2s, c = 'g')\n    subplots[1, 0].set_title(\"chi2\")\n    subplots[1, 1].plot(pvars, c = 'g')\n    subplots[1, 1].set_title(\"pvar\")\n\n    figure.canvas.draw()\n    figure.canvas.flush_events()\n\n    return figure, subplots, plots\n\n\ndef network_train(neurons, J_in, H_in, P_in, experimental_data, sim_param, plot = False):\n    J = J_in.copy()\n    H = H_in.copy()\n    P = P_in.copy()\n    state = None\n    chi2s = []\n    pvars = []\n    i = 0\n    figure, subplots, plots = None, None, {}\n    random_neuron_index = npr.choice(target_neurons)\n    while i < 500:\n        state, J, P, chi2 = run(neurons, J, H, P, experimental_data, sim_param)\n        distance = np.linalg.norm(experimental_data - state[:, [i*5 for i in range(experimental_data.shape[1])]])\n        pvar = 1 - (distance / (math.sqrt(300 * 1200)) * np.std(experimental_data)) ** 2\n        chi2s.append(chi2)\n        pvars.append(pvar)\n\n        if plot:\n            figure, subplots, plots = plot_train_evolution(\n                    figure,\n                    subplots,\n                    chi2s,\n                    pvars,\n                    random_neuron_index,\n                    state,\n                    experimental_data,\n                    plots,\n                    )\n\n        i += 1\n    return state, J, P, chi2s, pvars\n\n\n#|%%--%%| <54L3Vlhdwl|6gHScWGyvc>\n\nsimulated_data, final_J, final_P, chi2s, pvars = network_train(\n        neurons,\n        J,\n        H,\n        P,\n        experimental_data,\n        sim_param,\n        plot = True\n        )\n\n#|%%--%%| <6gHScWGyvc|ieMJ1JgqoE>\nr\"\"\"°°°\n# Analysis of the results\nHaving now provided a framework for building an in-silico replica of the dynamics of the neurons recorded during an experiment, we'll explore what properties can be inferred from the simulation.\n°°°\"\"\"\n#|%%--%%| <ieMJ1JgqoE|2gWTGSRMtN>\nr\"\"\"°°°\n## CURBD - Current based decomposition of multi-region datasets\nThe current into any one target neurons is the sum of the activities of all the neurons in the network scaled by the respective interaction weights:\n\n$$I_{target} = \\sum_{source} J_{source, target} \\cdot activity_{source} = \\sum_{source} J_{source, target} \\cdot \\phi(I_{source})$$\n\nWhen applied to the whole network this equation can reconstruct the full activity of units in the target regions.\nMoreover the equation can be restricted to source  units from a specific region, so to isolate the currents into the target region from a specific source one.\n\nThis is done by dividing the matrix $J$ into $M^2$ sub-matrices, where $M$ is the number of regions identified in the dataset (which for now should be known *a priori* for the scope of this discussion).\n\nThe separation of currents can be considered as a decomposition of the activity of the target-region neurons based on the relative contributions of each source one.\n°°°\"\"\"\n#|%%--%%| <2gWTGSRMtN|fqZf3T5pfG>\n\nregions = {\n        'A' : np.arange(\n            0,\n            data_params['n_neurons_A']\n            ),\n        'B' : np.arange(\n            data_params['n_neurons_A'],\n            data_params['n_neurons_A'] + data_params['n_neurons_B']\n            ),\n        'C' : np.arange(\n            data_params['n_neurons_A'] + data_params['n_neurons_B'],\n            data_params['n_neurons_A'] + data_params['n_neurons_B'] + data_params['n_neurons_C']\n            ),\n        }\n\n#|%%--%%| <fqZf3T5pfG|tVxTVrs8n5>\n\ndef curbd(simulated_activity, J, regions, current_type):\n    new_J = J.copy()\n\n    if current_type == 'all':\n        pass\n    elif current_type == 'excitatory':\n        new_J[new_J < 0] = 0\n    elif current_type == 'inhibitory':\n        new_J[new_J > 0] = 0\n    else:\n        raise ValueError(\"current_type must be 'all', 'excitatory' or 'inhibitory', not \" + f\"'{current_type}'\")\n\n    curbd = {}\n\n    for target in regions:\n        for source in regions:\n            sub_J = new_J[regions[source], :][:, regions[target]]\n            curbd[f\"{source} {target}\"] = sub_J.dot(simulated_activity[regions[source], :])\n\n    return curbd\n\n\n#|%%--%%| <tVxTVrs8n5|j4Cqxy8nuX>\n\ncurbd_res = curbd(test, final_J, regions, current_type = 'all')\n\nfigure, subplot = plt.subplots(len(regions), len(regions))\nfor (i, region) in enumerate(curbd_res):\n    subplot_col = i // len(regions)\n    subplot_row = i % len(regions)\n    subplot[subplot_col, subplot_row].pcolormesh(\n            curbd_res[region],\n            )\n    subplot[subplot_col, subplot_row].set_xlabel(\"Time (s)\")\n    subplot[subplot_col, subplot_row].set_ylabel(f\"Neurons in {region.split()[-1]}\")\n    subplot[subplot_col, subplot_row].set_title(f\"Current into {region.split()[-1]} from {region.split()[0]}\")\n\n#|%%--%%| <j4Cqxy8nuX|N7QJYzw5Zo>\nr\"\"\"°°°\n## State space analysis\nThe state space analysis is based on principal component analysis and allows to describe the istantaneous network state.\n\nThe output of the training is fitted in a PCA and the eigenvalues of the corresponding matrix, expressed as a fraction of their sum indicate the distribution of variance across different orthogonal directions in the active trajectory.\n\nThis allows to define an effective dimensionality of the activity $N_{eff}$ as the number of prncipal components that capture $90\\%$ of the variance in the dynamics.\n°°°\"\"\"\n#|%%--%%| <N7QJYzw5Zo|kXwB3kD25H>\n\nn_components = 50\n\npca = PCA(n_components)\npca.fit(simulated_data.T)\nproj = pca.transform(simulated_data.T)\ncum_explained_variance_ratio_simulated = pca.explained_variance_ratio_.cumsum()\nfigure, subplot = plt.subplots(1, 2, subplot_kw = {'projection' : '3d'})\nsubplot[0].plot(proj[:, 0], proj[:, 1], proj[:, 2])\n\npca = PCA(n_components)\npca.fit(experimental_data.T)\nproj = pca.transform(experimental_data.T)\ncum_explained_variance_ratio_experimental = pca.explained_variance_ratio_.cumsum()\n\nsubplot[1].plot(proj[:, 0], proj[:, 1], proj[:, 2])\n\n#|%%--%%| <kXwB3kD25H|HRnhrkYEtj>\n\nfigure, subplot = plt.subplots(1, 2)\nsubplot[0].plot(cum_explained_variance_ratio_simulated)\nsubplot[0].axhline(0.9, color = 'red', ls = '--')\nsubplot[1].plot(cum_explained_variance_ratio_experimental)\nsubplot[1].axhline(0.9, color = 'red', ls = '--')\n\n#|%%--%%| <HRnhrkYEtj|VaRbUleOkX>\n\nsim = np.searchsorted(cum_explained_variance_ratio_simulated, 0.9)\nexp = np.searchsorted(cum_explained_variance_ratio_experimental, 0.9)\nprint(f\"Number of principal components in the simulation such that the captured variance is greater than 90%: {sim}\")\nprint(f\"Number of principal components in the experiment such that the captured variance is greater than 90%: {exp}\")\n\n#|%%--%%| <VaRbUleOkX|MOr4UkMIAG>\nr\"\"\"°°°\n## Effective connectivity\nThe matrix $J$ of connectivity strenghts allow to describe the strength of connectivity between regions.\n\nPlotting the hitogram of values of $J$ in a specific sub-region allow to infer how they are connected.\n°°°\"\"\"\n#|%%--%%| <MOr4UkMIAG|BRyO520dwq>\n\n\nimport seaborn as sns\n\n\nfigure, subplot = plt.subplots(len(regions), len(regions), sharex = True, sharey = True)\nplt.suptitle(\"Before training\")\nfor (i, source) in enumerate(regions):\n    for (j, target) in enumerate(regions):\n        sns.distplot(J[regions[source], :][:, regions[target]],\n                     hist=True,\n                     kde=True,\n                     bins=200,\n                     color = 'darkblue',\n                     hist_kws={'edgecolor':'black'},\n                     kde_kws={'linewidth': 4},\n                        ax=subplot[i, j])\n        subplot[i, j].set_title(f\"{source} -> {target}\")\n        subplot[i, j].set_yscale('log')\n\n#|%%--%%| <BRyO520dwq|gl5hbPUQEU>\n\n\nfigure, subplot = plt.subplots(len(regions), len(regions), sharex = True, sharey = True)\nfigure.suptitle(\"After training\")\nfor (i, source) in enumerate(regions):\n    for (j, target) in enumerate(regions):\n        sns.distplot(final_J[regions[source], :][:, regions[target]],\n                     hist=True,\n                     kde=True,\n                     bins=200,\n                     color = 'darkblue',\n                     hist_kws={'edgecolor':'black'},\n                     kde_kws={'linewidth': 4},\n                        ax=subplot[i, j])\n        subplot[i, j].set_title(f\"{source} -> {target}\")\n        subplot[i, j].set_yscale('log')\nsubplots[2].plot(\n        np.arange(0, sim_param['sim_time'], sim_param['data_dt']),\n        experimental_data[random_neuron_index, :],\n        label = 'experimental'\n        )", "cmd_opts": " -s --md_cell_start=r\\\"\\\"\\\"°°°", "import_complete": 1, "terminal": "nvimterm"}